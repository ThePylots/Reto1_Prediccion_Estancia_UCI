{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25714e48-9e08-4546-b56d-ae3a4ef667c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nevergrad in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (0.6.0)\n",
      "Requirement already satisfied: numpy>=1.21.1 in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from nevergrad) (1.24.2)\n",
      "Requirement already satisfied: cma>=2.6.0 in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from nevergrad) (3.3.0)\n",
      "Requirement already satisfied: pandas in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from nevergrad) (1.5.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from nevergrad) (4.5.0)\n",
      "Requirement already satisfied: bayesian-optimization>=1.2.0 in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from nevergrad) (1.4.2)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from bayesian-optimization>=1.2.0->nevergrad) (1.10.1)\n",
      "Requirement already satisfied: colorama>=0.4.6 in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from bayesian-optimization>=1.2.0->nevergrad) (0.4.6)\n",
      "Requirement already satisfied: scikit-learn>=0.18.0 in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from bayesian-optimization>=1.2.0->nevergrad) (1.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from pandas->nevergrad) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from pandas->nevergrad) (2022.7.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas->nevergrad) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from scikit-learn>=0.18.0->bayesian-optimization>=1.2.0->nevergrad) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from scikit-learn>=0.18.0->bayesian-optimization>=1.2.0->nevergrad) (3.1.0)\n",
      "Requirement already satisfied: yellowbrick in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (1.5)\n",
      "Requirement already satisfied: cycler>=0.10.0 in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from yellowbrick) (0.11.0)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from yellowbrick) (1.24.2)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.2 in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from yellowbrick) (3.7.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from yellowbrick) (1.2.2)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from yellowbrick) (1.10.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (1.4.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (4.39.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (1.0.7)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (5.12.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (9.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (23.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (2.8.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from scikit-learn>=1.0.0->yellowbrick) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from scikit-learn>=1.0.0->yellowbrick) (3.1.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib!=3.0.0,>=2.0.2->yellowbrick) (3.15.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib!=3.0.0,>=2.0.2->yellowbrick) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nevergrad\n",
    "!pip install yellowbrick\n",
    "\n",
    "import sqlite3 as db\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import statistics as stat\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nevergrad as ng\n",
    "\n",
    "from copy import deepcopy\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "import yellowbrick\n",
    "from yellowbrick.regressor import prediction_error, PredictionError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1707430-21a2-46c7-8200-85c0b1d0396e",
   "metadata": {
    "tags": []
   },
   "source": [
    "<center><h2><b>Leer DB</b></h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4537b5d-91a0-4109-9007-164027b5d41f",
   "metadata": {},
   "source": [
    "Primero de todo leemos la base de datos en un dataframe. Hemos limpiado cada tabla individualmente y las hemos juntado con LEFT JOIN para que no se pierda<br>\n",
    "ninguna información de la tabla principal que es pacient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647c573a-5a64-44b1-b78f-4640e98cf1a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sql_query(q):\n",
    "    conn = db.connect('../db/sqlite/eicu_v2_0_1_clean.sqlite3')\n",
    "    df = pd.read_sql_query(q, conn)\n",
    "    conn.close()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a86ef2-77f5-44c1-ba7d-11928a247dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_query = \"\"\"\n",
    "    SELECT *\n",
    "    FROM patient P LEFT JOIN diagnosis                    D ON P.patientunitstayid = D.patientunitstayid\n",
    "                   LEFT JOIN admissiondrug               AD ON P.patientunitstayid = AD.patientunitstayid\n",
    "                   LEFT JOIN respiratoryCare             RC ON P.patientunitstayid = RC.patientunitstayid\n",
    "                   LEFT JOIN physicalExam                PE ON P.patientunitstayid = PE.patientunitstayid\n",
    "                   LEFT JOIN admissionDx                ADX ON P.patientunitstayid = ADX.patientunitstayid\n",
    "                   LEFT JOIN carePlanCareProvider         C ON P.patientunitstayid = C.patientunitstayid\n",
    "                   LEFT JOIN carePlanGeneral             CG ON P.patientunitstayid = CG.patientunitstayid\n",
    "                   LEFT JOIN carePlanInfectiousDisease CGID ON P.patientunitstayid = CGID.patientunitstayid\n",
    "                   LEFT JOIN carePlanGoal               CPG ON P.patientunitstayid = CPG.patientunitstayid\n",
    "                   LEFT JOIN vitalAperiodic             VAP ON P.patientunitstayid = VAP.patientunitstayid\n",
    "                   LEFT JOIN vitalPeriodic               VP ON P.patientunitstayid = VP.patientunitstayid\n",
    "                   LEFT JOIN medication                   M ON P.patientunitstayid = M.patientunitstayid\n",
    "                   LEFT JOIN allergy                     AL ON P.patientunitstayid = AL.patientunitstayid\n",
    "                   LEFT JOIN infusiondrug                ID ON P.patientunitstayid = ID.patientunitstayid\n",
    "\"\"\"\n",
    "\n",
    "X = sql_query(X_query).drop(columns=['patientunitstayid'])\n",
    "y = X['unitdischargeoffset']\n",
    "\n",
    "X['apacheadmissiondx'] = X['apacheadmissiondx'].fillna('_Unknown')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210d6a0d-9f62-46e0-b810-00e7620c76e0",
   "metadata": {},
   "source": [
    "<center><h2><b>Sanear valores dejados a NaN del Left Join</b></h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2fbddf-8bed-4a08-918a-004e5b884a91",
   "metadata": {},
   "source": [
    "Al hacer Left Join nos quedan campos nulos. Dependiendo del tipo de datos y de los otros datos de cada columna en particular<br>\n",
    "los rellenamos con 0, -1 o un string indicando falta de valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1d05e3-441f-49d0-8797-44df7bc23bcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# admissiondrug\n",
    "X['currenthistoryseqnum'] = X['currenthistoryseqnum'].fillna('_Unknown')\n",
    "for col in ['ASPIRIN', 'LISINOPRIL', 'LASIX']:\n",
    "    X[col] = X[col].fillna(0.0)\n",
    "\n",
    "# physicalExam\n",
    "for col in ['BPD_Current', 'BPD_Highest', 'BPD_Lowest', 'BPS_Current', 'BPS_Highest', 'BPS_Lowest', 'Blood_Loss', 'Dialysis_Net', 'O2Sat_Current',\n",
    "            'O2Sat_Highest', 'O2Sat_Lowest', 'Urine', 'Intubated', 'Comatose', 'Ventilated', 'Motor', 'Verbal', 'Eyes']:\n",
    "    X[col] = X[col].fillna(-1)\n",
    "\n",
    "# admissionDx\n",
    "for col in ['Cardiovascular', 'Respiratory', 'Neurologic']:\n",
    "    X[col] = X[col].fillna(0)\n",
    "\n",
    "# carePlanGoal\n",
    "for col in ['Care_Patient_Family', 'Care_Pulmonary', 'Care_Fluid_Balance_Treatments', 'Care_Activity_Safety',\n",
    "            'Care_Cardiovascular', 'Care_Infection_Labs']:\n",
    "    X[col] = X[col].fillna(0)\n",
    "\n",
    "# carePlanCareProvider\n",
    "for col in ['Categoria_Hospital', 'Categoria_Cardiology', 'Categoria_Internal_Medicine', 'Intervencion_I', 'Intervencion_II', 'Intervencion_III', 'Intervencion_IV']:\n",
    "    X[col] = X[col].fillna(0)\n",
    "\n",
    "# carePlanGeneral\n",
    "for col in ['Categoria_Ventilacion', 'Categoria_DVT_Prophylaxis', 'Categoria_Airway', 'Categoria_Care_Limitation', 'Categoria_Stress_Ulcer_Prophylaxis']:\n",
    "    X[col] = X[col].fillna(0)\n",
    "\n",
    "# carePlanInfectiousDisease\n",
    "for col in ['infectdiseasesite', 'infectdiseaseassessment']:\n",
    "    X[col] = X[col].fillna('_None')\n",
    "\n",
    "# vitalAperiodic\n",
    "X['last_aperiodic_off'] = X['last_aperiodic_off'].fillna(-1)\n",
    "for col in ['last_aperiodic_systolic', 'last_aperiodic_diastolic', 'last_aperiodic_mean']:\n",
    "    X[col] = X[col].fillna(X[col].median())\n",
    "    \n",
    "# vitalPeriodic\n",
    "for col in ['temperature', 'sao2', 'respiration', 'cvp', 'heartrate']:\n",
    "    X[col] = X[col].fillna(X[col].median())\n",
    "\n",
    "# Medication\n",
    "X['last_order_offset'] = X['last_order_offset'].fillna(-1)\n",
    "\n",
    "# Allergy\n",
    "for col in ['nDrugsAllergic', 'nNondrugsAllergic', 'totalAllergic']:\n",
    "    X[col] = X[col].fillna(0)\n",
    "    X[col] = X[col].astype('int32')\n",
    "    \n",
    "# Diagnosis\n",
    "for col in ['last1', 'last2', 'last3', 'last4']:\n",
    "    X[col] = X[col].fillna('_Unknown')\n",
    "for col in ['last1_off', 'last2_off', 'last3_off', 'last4_off']:\n",
    "    X[col] = X[col].fillna(-1)\n",
    "\n",
    "# InfusionDrug\n",
    "X['lastInfusionDrugOffset'] = X['lastInfusionDrugOffset'].fillna(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ab8ba1-0624-448a-bada-e1ed2d007ab6",
   "metadata": {},
   "source": [
    "Tras el paso anterior, este es el dataset resultante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e8a661-fee8-4130-8b40-b942e2bfef4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 2000)\n",
    "pd.set_option('display.max_columns', 2000)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 400)\n",
    "X.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2302ffb-bafe-4daf-a6b1-0242111a4beb",
   "metadata": {},
   "source": [
    "<center><h2><b>Transformación de columnas</b></h2></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50067d2-ff8f-4d85-b862-4bcaa5661469",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "categs = {}\n",
    "\n",
    "for col in ['gender', 'ethnicity', 'hospitalid', 'hospitaladmitsource', 'currenthistoryseqnum', 'infectdiseasesite', 'infectdiseaseassessment',\n",
    "            'nDrugsAllergic', 'nNondrugsAllergic', 'totalAllergic']:\n",
    "    categs[col] = list(set(X[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c58882-a94e-4f58-8c30-926bf46fa6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=234750)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f800a6-a0e1-437c-8148-7c181ff5163e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Condicionar el test según la edad del paciente\n",
    "\n",
    "#X_test = X_test[X_test['age'] >= 60]\n",
    "#y_test = y_test[y_test.index.isin(X_test.index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c78768-4a8e-47ea-a2a9-07601d0b7b55",
   "metadata": {
    "tags": []
   },
   "source": [
    "No hemos puesto demasiada antención a la eliminación de outliers y normalización de columnas numéricas ya que nuestro principal regresor ha sido RandomForest. Hemos usado<br>\n",
    "OneHot para las variables categóricas con un número limitado de columnas y directamente passthrough para las variables numéricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1919db-da40-4437-afd1-1c6d49731730",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transformers = [\n",
    "    # Patient\n",
    "    ('gender',                  OneHotEncoder(categories=[categs['gender']]),  []),\n",
    "    ('age',                     'passthrough',    []),\n",
    "    ('ethnicity',               OneHotEncoder(categories=[categs['ethnicity']]),  []),\n",
    "    ('hospitalid',              OneHotEncoder(categories=[categs['hospitalid']]),  []),\n",
    "    ('apacheadmissiondx',       'drop',           []),\n",
    "    ('admissionheight',         'passthrough',    []),\n",
    "    ('hospitaladmitoffset',     'passthrough',    []),\n",
    "    ('hospitaladmitsource',      OneHotEncoder(categories=[categs['hospitaladmitsource']]), []),\n",
    "    ('hospitaldischargeoffset', 'drop',           []),\n",
    "    ('unitvisitnumber',         'passthrough',    []),    \n",
    "    ('admissionweight',         'passthrough',    []),\n",
    "    ('unitdischargeoffset',     'drop',           []),\n",
    "    ('_avg_unit_stay',          'passthrough',    []),\n",
    "    ('_avg_hospital_stay',      'passthrough',    []),\n",
    "    ('_admission_bmi',          'passthrough',    []),\n",
    "    \n",
    "    # Diagnosis\n",
    "    ('_DIAGNOSIS_last1', 'drop', []),\n",
    "    ('_DIAGNOSIS_last2', 'drop', []),\n",
    "    ('_DIAGNOSIS_last3', 'drop', []),\n",
    "    ('_DIAGNOSIS_last4', 'drop', []),\n",
    "\n",
    "\n",
    "    ('_DIAGNOSIS_last1_off', 'passthrough', []),\n",
    "    ('_DIAGNOSIS_last2_off', 'passthrough', []),\n",
    "    ('_DIAGNOSIS_last3_off', 'passthrough', []),\n",
    "    ('_DIAGNOSIS_last4_off', 'passthrough', []),\n",
    "    \n",
    "    # AdmissionDrug\n",
    "    ('_admissionAspirin',    'passthrough', []),\n",
    "    ('_admissionLisinopril', 'passthrough', []),\n",
    "    ('_admissionLasix',      'passthrough', []),\n",
    "    \n",
    "    # RespCare\n",
    "    ('currenthistoryseqnum', OneHotEncoder(categories=[categs['currenthistoryseqnum']]), []),\n",
    "    \n",
    "    # PhysicalExam\n",
    "    # TODO\n",
    "    ('a0', 'drop', []),\n",
    "    ('a1', 'drop', []),\n",
    "    ('a2', 'drop', []),\n",
    "    ('a3', 'drop', []),\n",
    "    ('a4', 'drop', []),\n",
    "    ('a5', 'drop', []),\n",
    "    ('a6', 'drop', []),\n",
    "    ('a7', 'drop', []),\n",
    "    ('a8', 'drop', []),\n",
    "    ('a9', 'drop', []),\n",
    "    ('b1', 'drop', []),    \n",
    "    ('b2', 'drop', []),\n",
    "    ('b3', 'drop', []),\n",
    "    ('b4', 'drop', []),\n",
    "    ('b5', 'drop', []),\n",
    "    ('b6', 'drop', []),\n",
    "    ('b7', 'drop', []),\n",
    "    ('b8', 'drop', []),\n",
    "\n",
    "    # AdmissionDx\n",
    "    ('_admissionCardiovascular', 'passthrough',  []),\n",
    "    ('_admissionRespiratory',    'passthrough',  []),\n",
    "    ('_admissionNeurologic',     'passthrough',  []),\n",
    "  \n",
    "    # CarePlanProvider\n",
    "    ('Categoria_Hospital',          'passthrough', []), \n",
    "    ('Categoria_Cardiology',        'passthrough', []),\n",
    "    ('Categoria_Internal_Medicine', 'passthrough', []),\n",
    "    ('Intervencion_I',              'passthrough', []),\n",
    "    ('Intervencion_II',             'passthrough', []),\n",
    "    ('Intervencion_III',            'passthrough', []), \n",
    "    ('Intervencion_IV',             'passthrough', []),\n",
    "    \n",
    "    # CarePlanGeneral\n",
    "    ('Categoria_Ventilacion',              'passthrough', []),\n",
    "    ('Categoria_DVT_Prophylaxis',          'passthrough', []),\n",
    "    ('Categoria_Airway',                   'passthrough', []),\n",
    "    ('Categoria_Care_Limitation',          'passthrough', []),\n",
    "    ('Categoria_Stress_Ulcer_Prophylaxis', 'passthrough', []),\n",
    "    \n",
    "    # CarePlanInfectiousDisease\n",
    "    ('infectdiseasesite',      OneHotEncoder(categories=[categs['infectdiseasesite']]), []),\n",
    "    ('infectdiseaseassessment', OneHotEncoder(categories=[categs['infectdiseaseassessment']]), []),\n",
    "    \n",
    "    ('last_aperiodic_off',       'passthrough', []),\n",
    "    ('last_aperiodic_systolic',  'passthrough', []),\n",
    "    ('last_aperiodic_diastolic', 'passthrough', []),\n",
    "    ('last_aperiodic_mean',      'passthrough', []),\n",
    "    \n",
    "    # vitalPeriodic\n",
    "    ('VP_temp',        'passthrough', []),\n",
    "    ('VP_sao2',        'passthrough', []),\n",
    "    ('VP_respiration', 'passthrough', []),\n",
    "    ('VP_cvp',         'passthrough', []),\n",
    "    ('VP_heartrate',   'passthrough', []),\n",
    "\n",
    "    # Medication\n",
    "    ('last_order_offset', 'passthrough', []),\n",
    "    \n",
    "    # Allergy\n",
    "    # TODO\n",
    "    ('nDrugsAllergic',    'drop', []),#OneHotEncoder(categories=[categs['nDrugsAllergic']]), []),\n",
    "    ('nNondrugsAllergic', 'drop', []),#OneHotEncoder(categories=[categs['nNondrugsAllergic']]), []),\n",
    "    ('totalAllergic',     'drop', []),#OneHotEncoder(categories=[categs['totalAllergic']]), []),\n",
    "    \n",
    "    # TODO\n",
    "    ('asdofibasodfboasdfdsf',  'passthrough', []),\n",
    "    \n",
    "]\n",
    "\n",
    "# Numerar columnas para el tranformador\n",
    "for i in range(len(transformers)):\n",
    "    transformers[i][2].append(i)\n",
    "\n",
    "# Transformar la matriz\n",
    "X_T_train = ColumnTransformer(transformers=transformers).fit_transform(X_train)\n",
    "X_T_test  = ColumnTransformer(transformers=transformers).fit_transform(X_test)\n",
    "\n",
    "# Mostrar el cambio en columnas\n",
    "print('Train', X.shape, '->', X_T_train.shape)\n",
    "print('Test ', X_test.shape, ' ->', X_T_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1df4b7-8b4f-4314-8da5-b3fa9c31b62a",
   "metadata": {},
   "source": [
    "<center><h2><b>Busqueda de Hiperparametros</b></h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eee9c46-79c8-49c9-8391-29311f61e5d3",
   "metadata": {
    "tags": []
   },
   "source": [
    "Hemos usado Nevergrad para buscar los hiperparámetros de RandomForest y XBoost mediante un algorítmo genético, ya que son una cantidad importante de hiperparámetros.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f505075b-f1c9-4775-8468-f720c6b76ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Número aleatorio para la búsqueda de hiperparámetros\n",
    "rand_n = random.randint(0, 100000000000)\n",
    "\n",
    "# Funcion auxiliar para devolver un conjunto de scores y su media y desviación estandar\n",
    "def cv_avg_std(reg, X, y, scoring):\n",
    "    maes = cross_val_score(reg, X, y, cv=5, scoring=scoring)\n",
    "    avg = stat.mean(maes)\n",
    "    std_dev = stat.variance(maes)**(1/2)\n",
    "    \n",
    "    return maes, avg, std_dev\n",
    "\n",
    "# Función de optimización para nevergrad\n",
    "def optimize_spectral(n_estimators, min_samples_leaf, min_samples_split, max_depth, max_features, warm_start, bootstrap):\n",
    "    reg = RandomForestRegressor(n_estimators=n_estimators, min_samples_leaf=min_samples_leaf,\n",
    "                              min_samples_split=min_samples_split, max_depth=max_depth, random_state=rand_n,\n",
    "                              max_features=max_features, bootstrap=bootstrap)\n",
    "\n",
    "    maes, avg, std_dev = cv_avg_std(reg, X_T_train, y_train, 'neg_mean_absolute_error')\n",
    "    _, r2, __ = cv_avg_std(reg, X_T, y, 'r2')\n",
    "  \n",
    "    print('AVG_MAE', avg, 'R2', r2, 'std_dev', std_dev, \"[\", n_estimators, \",\", min_samples_leaf, \",\", min_samples_split, \",\", max_depth, ',', max_features, ',', warm_start, ',', bootstrap, '] - ', rand_n)\n",
    "\n",
    "    return float('inf') if std_dev > 75 else -r2\n",
    "\n",
    "instru = ng.p.Instrumentation(\n",
    "    ng.p.Choice([x for x in range(40, 200)]), # n_estimators\n",
    "    ng.p.Choice([x for x in range(1, 4)]), # min_samples_leaf\n",
    "    ng.p.Choice([x for x in range(2, 5)]), # min_samples_split\n",
    "    ng.p.Choice([x for x in range(30, 200)]), # max_depth\n",
    "    ng.p.Choice([x for x in range(10, 200)]), # max_features\n",
    "    ng.p.Choice([True, False]), # warm_start\n",
    "    ng.p.Choice([True, False]) # bootstrap\n",
    ")\n",
    "\n",
    "# Descomentar para ejecutar\n",
    "\n",
    "#optimizer = ng.optimizers.CM(parametrization=instru, budget=1000)\n",
    "#recommendation = optimizer.minimize(optimize_spectral)\n",
    "#print(recommendation.value)  # recommended value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e91faa-9481-49ca-a7ba-e48e37cf9def",
   "metadata": {},
   "source": [
    "<center><h2><b>Reducción dimensional</b></h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2524d57e-845e-4fd0-93a0-e6bf4cd132df",
   "metadata": {
    "tags": []
   },
   "source": [
    "Hemos probado SVD para reducir el número de features, acelerar el tiempo de entreno y reducir el ruido que se haya podido originar<br>\n",
    "por parte de features que no sean demasiados útiles, ya que nuestro conjunto de training es una matriz esparsa y no se puede usar PCA,<br>\n",
    "sin embargo, no ha ayudado a mejorar el modelo, incrementando en su vez todas las medidas de error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7760a0-827c-4de4-95b0-d400492d2a03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def feature_n_for_explained_variance_ratio(svd, ratio):\n",
    "    total = 0\n",
    "    \n",
    "    for i, n in enumerate(svd.explained_variance_ratio_):\n",
    "        total += n\n",
    "        \n",
    "        if total >= ratio:\n",
    "            return i+1\n",
    "    \n",
    "    return -1\n",
    "\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=22)\n",
    "_X_T_train = svd.fit_transform(X_T_train)\n",
    "\n",
    "for n in (0.5, 0.75, 0.9, 0.99, 0.999, 0.9999):\n",
    "    print(\"Numero de features para explicar un \" + str(n*100) + \"% de la varianza:\",\n",
    "          feature_n_for_explained_variance_ratio(svd, n))\n",
    "\n",
    "#X_T = svd.transform(X_T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9209c1e-dcc1-48f3-a9c2-ee1fda740e38",
   "metadata": {
    "tags": []
   },
   "source": [
    "<center><h2><b>Entrenamiento y calcular Error</b></h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96666a24-5ac2-4dbf-a7ca-68a31b809077",
   "metadata": {},
   "source": [
    "Para el cálculo de error hemos usado tres medidas. Por una parte tenemos el MAE para calcular la distancia media a la variable objetivo que es el<br>\n",
    "número de minutos que tenemos que predecir que un paciente va a permanecer en la UCI. Aparte del MAE también hemos considerado importante la desviación<br>\n",
    "estandar del mismo ya que se trata de un sistema donde se pueden perder vidas si se hacen predicciones poco precisas. Por último hemos usado R2 para<br>\n",
    "calcular la cantidad de varianza que explica el modelo y evitar overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbff6c5b-7009-4365-8195-93d253660201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg df: 3484.0752 = 2.420139 dias.\n",
    "def cv_avg_std(reg, X, y, scoring):\n",
    "    maes = cross_val_score(reg, X, y, cv=2, scoring=scoring)\n",
    "    avg = stat.mean(maes)\n",
    "    std_dev = stat.variance(maes)**(1/2)\n",
    "    \n",
    "    return maes, avg, std_dev\n",
    "\n",
    "def make_df(datos_reg):\n",
    "    error_df = pd.DataFrame()\n",
    "\n",
    "    error_df['Regresor']               = datos_reg.keys()\n",
    "    error_df['Average MAE']            = [ abs(dato['avg']) for dato in datos_reg.values() ]\n",
    "    error_df['Standard Deviation MAE'] = [ dato['std_dev'] for dato in datos_reg.values() ]\n",
    "    error_df['Average R2']             = [ dato['avg_r2'] for dato in datos_reg.values() ]\n",
    "    error_df['time']                   = [ dato['time'] for dato in datos_reg.values() ]\n",
    "    \n",
    "    return error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3851ab3e-016b-4678-816d-221e81deea67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Linear regression \n",
    "lreg = LinearRegression()\n",
    "\n",
    "rfreg = RandomForestRegressor(\n",
    "        n_estimators = 121,\n",
    "        min_samples_leaf = 2,\n",
    "        min_samples_split = 3,\n",
    "        max_depth = 91,\n",
    "        max_features = 189,\n",
    "        warm_start = False,\n",
    "        bootstrap = False,\n",
    "        random_state = 14684358)\n",
    "\n",
    "xboostreg = XGBRegressor()\n",
    "\n",
    "votingreg = VotingRegressor([('xboost', lreg), ('rfreg', rfreg)], weights=[1, 3])\n",
    "\n",
    "datos_reg = {}\n",
    "regressors = [\n",
    "    ('Lineal', lreg),\n",
    "    ('Random Forest', rfreg),\n",
    "    ('XGBRegressor', xboostreg),\n",
    "    ('VotingRegressor', votingreg),\n",
    "]\n",
    "\n",
    "# Medir tiempo y hacer predicciones para cada regresor\n",
    "for reg_name, reg in regressors:\n",
    "    start_time = time.time()\n",
    "\n",
    "    maes, avg, std_dev = cv_avg_std(reg, X_T_train, y_train, 'neg_mean_absolute_error')\n",
    "    maes, r2, _ = cv_avg_std(reg, X_T_train, y_train, 'r2')\n",
    "    \n",
    "    datos_reg[reg_name] = { 'avg': avg, 'std_dev': std_dev, 'time': time.time() - start_time }\n",
    "    datos_reg[reg_name]['avg_r2'] = r2\n",
    "\n",
    "df_reg = make_df(datos_reg)\n",
    "df_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1963dca0-6d5d-4399-96a5-b0a7a22ba6b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.set_palette(\"Paired\")\n",
    "fig, axs = plt.subplots(ncols=3, nrows=1,figsize=(20, 5))\n",
    "\n",
    "sns.barplot(data=df_reg, x='Regresor', y='Average MAE', ax=axs[0])\n",
    "sns.barplot(data=df_reg, x='Regresor', y='Standard Deviation MAE', ax=axs[1])\n",
    "sns.barplot(data=df_reg, x='Regresor', y='Average R2', ax=axs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e807b882-d8c7-4340-b163-9a3afb75fac3",
   "metadata": {},
   "source": [
    "Por último hacemos un plot con el error de cada modelo para comparar cuanto se distancian de un resultado ideal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9797b3b-863b-408e-acf6-5396d6396a3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rfreg.fit(X_T_train, y_train)\n",
    "pred = rfreg.predict(X_T_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad9de5d-730f-4b99-a3fa-a41e44b9a35b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# avg df: 3484.0752 = 2.420139 dias.\n",
    "mae = mean_absolute_error(y_test, pred)\n",
    "r2  = r2_score(y_test, pred)\n",
    "\n",
    "print('MAE test:', mae)\n",
    "print('R2:', r2, '\\n')\n",
    "\n",
    "err = prediction_error(votingreg,  X_T_train, y_train, X_T_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1827f23f-1382-45ea-b046-33ae71627c63",
   "metadata": {},
   "source": [
    "Basandonos en estos plots podemos ver que todos se acercan bastante a la pendiente de 45º. LinearRegressor y GBRegressor se<br>\n",
    "alejan un poco más, pero es natural ya que no hemos tenido tiempo de buscar unos hiperparámetros razonables o hacer eliminación<br>\n",
    "de outliers. En el caso de RandomForest no le afectan tanto estos outliers y se hace posible una explicación más amplia de la varianza."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe20616-c37b-43a9-bc7c-d0766612916e",
   "metadata": {},
   "source": [
    "---\n",
    "<center><h2><b>Conclusión</b></h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a865ba4d-3e70-4bf0-890b-5ecda6a6d9bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "Tras evaluar todos estos modelos podemos observar que mientras que LinearRegressor tiene un ligeramente mayor R2 y un tiempo de ejecución<br>\n",
    "envidiable, la diferencia en MAE no compensa usarlo. RandomForestRegressor se muestra como un contendiente fuerte, pero hemos podido mejorarlo<br>\n",
    "haciendo un ensemble de Voting con el anterior. XBoost probablemente mejoraría dado el tiempo suficiente para hacer una búsqueda de hiperparámetros<br>\n",
    "exhaustiva, pero no hemos tenido tiempo. A todo esto pensamos que el VotingRegressor muestra unos resultados aceptables.<br>\n",
    "\n",
    "En un nivel más abstracto podemos ver que tiene una media de error de ~262 minutos ~ 4 horas y una desviación estandar de alrededor de media hora. Creemos<br>\n",
    "que es importante contar con una desviación estandar pequeña para poder predecir la estancia de los pacientes con cierta fiabilidad y teniendo en cuenta<br>\n",
    "que el tiempo medio que un paciente está en la UCI en E.E.U.U es de alrededor de 10 días y que la media del dataset es de 2.4 días, creemos que es un<br>\n",
    "resultado interesante con el cual se podrían hacer, hipotéticamente, predicciones útiles.<br>\n",
    "\n",
    "También es merecedor de mención que este resultado no supera a APACHE. Un modelo únicamente entrenado con la variable unbridgedunitlos que tiene un índice<br>\n",
    "de correlación de más del 98% con la variable que estamos buscando tiene un R2 de un 98.5% y un MAE y desviación estandar menores. No hemos querido mejorar<br>\n",
    "el modelo hasta esos niveles tanto por falta de tiempo como por miedo de overfitting, aunque Random Forest es notorio por su tendencia a evitar overfitting.<br>\n",
    "\n",
    "En general y para terminar podemos decir que ha sido un reto interesante donde nuestros principales problemas han sido la interpretación de 30 tablas de datos<br>\n",
    "(muchas para la experiencia que habíamos tenido hasta ahora) y la interpretación de dichos datos, sobretodo los datos vacíos (donde algunos han acabado teniendo<br>\n",
    "sentido) como el significado de dichos datos en un contexto médico."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
