{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25714e48-9e08-4546-b56d-ae3a4ef667c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nevergrad in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (0.6.0)\n",
      "Requirement already satisfied: numpy>=1.21.1 in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from nevergrad) (1.24.2)\n",
      "Requirement already satisfied: cma>=2.6.0 in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from nevergrad) (3.3.0)\n",
      "Requirement already satisfied: pandas in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from nevergrad) (1.5.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from nevergrad) (4.5.0)\n",
      "Requirement already satisfied: bayesian-optimization>=1.2.0 in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from nevergrad) (1.4.2)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from bayesian-optimization>=1.2.0->nevergrad) (1.10.1)\n",
      "Requirement already satisfied: colorama>=0.4.6 in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from bayesian-optimization>=1.2.0->nevergrad) (0.4.6)\n",
      "Requirement already satisfied: scikit-learn>=0.18.0 in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from bayesian-optimization>=1.2.0->nevergrad) (1.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from pandas->nevergrad) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from pandas->nevergrad) (2022.7.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas->nevergrad) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from scikit-learn>=0.18.0->bayesian-optimization>=1.2.0->nevergrad) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from scikit-learn>=0.18.0->bayesian-optimization>=1.2.0->nevergrad) (3.1.0)\n",
      "Requirement already satisfied: yellowbrick in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (1.5)\n",
      "Requirement already satisfied: cycler>=0.10.0 in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from yellowbrick) (0.11.0)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from yellowbrick) (1.24.2)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.2 in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from yellowbrick) (3.7.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from yellowbrick) (1.2.2)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from yellowbrick) (1.10.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (1.4.4)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (4.39.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (1.0.7)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (5.12.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (9.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (23.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from matplotlib!=3.0.0,>=2.0.2->yellowbrick) (2.8.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from scikit-learn>=1.0.0->yellowbrick) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from scikit-learn>=1.0.0->yellowbrick) (3.1.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib!=3.0.0,>=2.0.2->yellowbrick) (3.15.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/alvaro/.config/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib!=3.0.0,>=2.0.2->yellowbrick) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nevergrad\n",
    "!pip install yellowbrick\n",
    "\n",
    "import sqlite3 as db\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import statistics as stat\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nevergrad as ng\n",
    "\n",
    "from copy import deepcopy\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "import yellowbrick\n",
    "from yellowbrick.regressor import prediction_error, PredictionError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1707430-21a2-46c7-8200-85c0b1d0396e",
   "metadata": {
    "tags": []
   },
   "source": [
    "<center><h2><b>Leer DB</b></h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4537b5d-91a0-4109-9007-164027b5d41f",
   "metadata": {},
   "source": [
    "Primero de todo leemos la base de datos en un dataframe. Hemos limpiado cada tabla individualmente y las hemos juntado con LEFT JOIN para que no se pierda<br>\n",
    "ninguna informaci√≥n de la tabla principal que es pacient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647c573a-5a64-44b1-b78f-4640e98cf1a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sql_query(q):\n",
    "    conn = db.connect('../db/sqlite/eicu_v2_0_1_clean.sqlite3')\n",
    "    df = pd.read_sql_query(q, conn)\n",
    "    conn.close()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a86ef2-77f5-44c1-ba7d-11928a247dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_query = \"\"\"\n",
    "    SELECT *\n",
    "    FROM patient P LEFT JOIN diagnosis                    D ON P.patientunitstayid = D.patientunitstayid\n",
    "                   LEFT JOIN admissiondrug               AD ON P.patientunitstayid = AD.patientunitstayid\n",
    "                   LEFT JOIN respiratoryCare             RC ON P.patientunitstayid = RC.patientunitstayid\n",
    "                   LEFT JOIN physicalExam                PE ON P.patientunitstayid = PE.patientunitstayid\n",
    "                   LEFT JOIN admissionDx                ADX ON P.patientunitstayid = ADX.patientunitstayid\n",
    "                   LEFT JOIN carePlanCareProvider         C ON P.patientunitstayid = C.patientunitstayid\n",
    "                   LEFT JOIN carePlanGeneral             CG ON P.patientunitstayid = CG.patientunitstayid\n",
    "                   LEFT JOIN carePlanInfectiousDisease CGID ON P.patientunitstayid = CGID.patientunitstayid\n",
    "                   LEFT JOIN carePlanGoal               CPG ON P.patientunitstayid = CPG.patientunitstayid\n",
    "                   LEFT JOIN vitalAperiodic             VAP ON P.patientunitstayid = VAP.patientunitstayid\n",
    "                   LEFT JOIN vitalPeriodic               VP ON P.patientunitstayid = VP.patientunitstayid\n",
    "                   LEFT JOIN medication                   M ON P.patientunitstayid = M.patientunitstayid\n",
    "                   LEFT JOIN allergy                     AL ON P.patientunitstayid = AL.patientunitstayid\n",
    "                   LEFT JOIN infusiondrug                ID ON P.patientunitstayid = ID.patientunitstayid\n",
    "\"\"\"\n",
    "\n",
    "X = sql_query(X_query).drop(columns=['patientunitstayid'])\n",
    "y = X['unitdischargeoffset']\n",
    "\n",
    "X['apacheadmissiondx'] = X['apacheadmissiondx'].fillna('_Unknown')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210d6a0d-9f62-46e0-b810-00e7620c76e0",
   "metadata": {},
   "source": [
    "<center><h2><b>Sanear valores dejados a NaN del Left Join</b></h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2fbddf-8bed-4a08-918a-004e5b884a91",
   "metadata": {},
   "source": [
    "Al hacer Left Join nos quedan campos nulos. Dependiendo del tipo de datos y de los otros datos de cada columna en particular<br>\n",
    "los rellenamos con 0, -1 o un string indicando falta de valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1d05e3-441f-49d0-8797-44df7bc23bcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# admissiondrug\n",
    "X['currenthistoryseqnum'] = X['currenthistoryseqnum'].fillna('_Unknown')\n",
    "for col in ['ASPIRIN', 'LISINOPRIL', 'LASIX']:\n",
    "    X[col] = X[col].fillna(0.0)\n",
    "\n",
    "# physicalExam\n",
    "for col in ['BPD_Current', 'BPD_Highest', 'BPD_Lowest', 'BPS_Current', 'BPS_Highest', 'BPS_Lowest', 'Blood_Loss', 'Dialysis_Net', 'O2Sat_Current',\n",
    "            'O2Sat_Highest', 'O2Sat_Lowest', 'Urine', 'Intubated', 'Comatose', 'Ventilated', 'Motor', 'Verbal', 'Eyes']:\n",
    "    X[col] = X[col].fillna(-1)\n",
    "\n",
    "# admissionDx\n",
    "for col in ['Cardiovascular', 'Respiratory', 'Neurologic']:\n",
    "    X[col] = X[col].fillna(0)\n",
    "\n",
    "# carePlanGoal\n",
    "for col in ['Care_Patient_Family', 'Care_Pulmonary', 'Care_Fluid_Balance_Treatments', 'Care_Activity_Safety',\n",
    "            'Care_Cardiovascular', 'Care_Infection_Labs']:\n",
    "    X[col] = X[col].fillna(0)\n",
    "\n",
    "# carePlanCareProvider\n",
    "for col in ['Categoria_Hospital', 'Categoria_Cardiology', 'Categoria_Internal_Medicine', 'Intervencion_I', 'Intervencion_II', 'Intervencion_III', 'Intervencion_IV']:\n",
    "    X[col] = X[col].fillna(0)\n",
    "\n",
    "# carePlanGeneral\n",
    "for col in ['Categoria_Ventilacion', 'Categoria_DVT_Prophylaxis', 'Categoria_Airway', 'Categoria_Care_Limitation', 'Categoria_Stress_Ulcer_Prophylaxis']:\n",
    "    X[col] = X[col].fillna(0)\n",
    "\n",
    "# carePlanInfectiousDisease\n",
    "for col in ['infectdiseasesite', 'infectdiseaseassessment']:\n",
    "    X[col] = X[col].fillna('_None')\n",
    "\n",
    "# vitalAperiodic\n",
    "X['last_aperiodic_off'] = X['last_aperiodic_off'].fillna(-1)\n",
    "for col in ['last_aperiodic_systolic', 'last_aperiodic_diastolic', 'last_aperiodic_mean']:\n",
    "    X[col] = X[col].fillna(X[col].median())\n",
    "    \n",
    "# vitalPeriodic\n",
    "for col in ['temperature', 'sao2', 'respiration', 'cvp', 'heartrate']:\n",
    "    X[col] = X[col].fillna(X[col].median())\n",
    "\n",
    "# Medication\n",
    "X['last_order_offset'] = X['last_order_offset'].fillna(-1)\n",
    "\n",
    "# Allergy\n",
    "for col in ['nDrugsAllergic', 'nNondrugsAllergic', 'totalAllergic']:\n",
    "    X[col] = X[col].fillna(0)\n",
    "    X[col] = X[col].astype('int32')\n",
    "    \n",
    "# Diagnosis\n",
    "for col in ['last1', 'last2', 'last3', 'last4']:\n",
    "    X[col] = X[col].fillna('_Unknown')\n",
    "for col in ['last1_off', 'last2_off', 'last3_off', 'last4_off']:\n",
    "    X[col] = X[col].fillna(-1)\n",
    "\n",
    "# InfusionDrug\n",
    "X['lastInfusionDrugOffset'] = X['lastInfusionDrugOffset'].fillna(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ab8ba1-0624-448a-bada-e1ed2d007ab6",
   "metadata": {},
   "source": [
    "Tras el paso anterior, este es el dataset resultante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e8a661-fee8-4130-8b40-b942e2bfef4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 2000)\n",
    "pd.set_option('display.max_columns', 2000)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 400)\n",
    "X.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2302ffb-bafe-4daf-a6b1-0242111a4beb",
   "metadata": {},
   "source": [
    "<center><h2><b>Transformaci√≥n de columnas</b></h2></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50067d2-ff8f-4d85-b862-4bcaa5661469",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "categs = {}\n",
    "\n",
    "for col in ['gender', 'ethnicity', 'hospitalid', 'hospitaladmitsource', 'currenthistoryseqnum', 'infectdiseasesite', 'infectdiseaseassessment',\n",
    "            'nDrugsAllergic', 'nNondrugsAllergic', 'totalAllergic']:\n",
    "    categs[col] = list(set(X[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c58882-a94e-4f58-8c30-926bf46fa6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=234750)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f800a6-a0e1-437c-8148-7c181ff5163e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Condicionar el test seg√∫n la edad del paciente\n",
    "\n",
    "#X_test = X_test[X_test['age'] >= 60]\n",
    "#y_test = y_test[y_test.index.isin(X_test.index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c78768-4a8e-47ea-a2a9-07601d0b7b55",
   "metadata": {
    "tags": []
   },
   "source": [
    "No hemos puesto demasiada antenci√≥n a la eliminaci√≥n de outliers y normalizaci√≥n de columnas num√©ricas ya que nuestro principal regresor ha sido RandomForest. Hemos usado<br>\n",
    "OneHot para las variables categ√≥ricas con un n√∫mero limitado de columnas y directamente passthrough para las variables num√©ricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1919db-da40-4437-afd1-1c6d49731730",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transformers = [\n",
    "    # Patient\n",
    "    ('gender',                  OneHotEncoder(categories=[categs['gender']]),  []),\n",
    "    ('age',                     'passthrough',    []),\n",
    "    ('ethnicity',               OneHotEncoder(categories=[categs['ethnicity']]),  []),\n",
    "    ('hospitalid',              OneHotEncoder(categories=[categs['hospitalid']]),  []),\n",
    "    ('apacheadmissiondx',       'drop',           []),\n",
    "    ('admissionheight',         'passthrough',    []),\n",
    "    ('hospitaladmitoffset',     'passthrough',    []),\n",
    "    ('hospitaladmitsource',      OneHotEncoder(categories=[categs['hospitaladmitsource']]), []),\n",
    "    ('hospitaldischargeoffset', 'drop',           []),\n",
    "    ('unitvisitnumber',         'passthrough',    []),    \n",
    "    ('admissionweight',         'passthrough',    []),\n",
    "    ('unitdischargeoffset',     'drop',           []),\n",
    "    ('_avg_unit_stay',          'passthrough',    []),\n",
    "    ('_avg_hospital_stay',      'passthrough',    []),\n",
    "    ('_admission_bmi',          'passthrough',    []),\n",
    "    \n",
    "    # Diagnosis\n",
    "    ('_DIAGNOSIS_last1', 'drop', []),\n",
    "    ('_DIAGNOSIS_last2', 'drop', []),\n",
    "    ('_DIAGNOSIS_last3', 'drop', []),\n",
    "    ('_DIAGNOSIS_last4', 'drop', []),\n",
    "\n",
    "\n",
    "    ('_DIAGNOSIS_last1_off', 'passthrough', []),\n",
    "    ('_DIAGNOSIS_last2_off', 'passthrough', []),\n",
    "    ('_DIAGNOSIS_last3_off', 'passthrough', []),\n",
    "    ('_DIAGNOSIS_last4_off', 'passthrough', []),\n",
    "    \n",
    "    # AdmissionDrug\n",
    "    ('_admissionAspirin',    'passthrough', []),\n",
    "    ('_admissionLisinopril', 'passthrough', []),\n",
    "    ('_admissionLasix',      'passthrough', []),\n",
    "    \n",
    "    # RespCare\n",
    "    ('currenthistoryseqnum', OneHotEncoder(categories=[categs['currenthistoryseqnum']]), []),\n",
    "    \n",
    "    # PhysicalExam\n",
    "    # TODO\n",
    "    ('a0', 'drop', []),\n",
    "    ('a1', 'drop', []),\n",
    "    ('a2', 'drop', []),\n",
    "    ('a3', 'drop', []),\n",
    "    ('a4', 'drop', []),\n",
    "    ('a5', 'drop', []),\n",
    "    ('a6', 'drop', []),\n",
    "    ('a7', 'drop', []),\n",
    "    ('a8', 'drop', []),\n",
    "    ('a9', 'drop', []),\n",
    "    ('b1', 'drop', []),    \n",
    "    ('b2', 'drop', []),\n",
    "    ('b3', 'drop', []),\n",
    "    ('b4', 'drop', []),\n",
    "    ('b5', 'drop', []),\n",
    "    ('b6', 'drop', []),\n",
    "    ('b7', 'drop', []),\n",
    "    ('b8', 'drop', []),\n",
    "\n",
    "    # AdmissionDx\n",
    "    ('_admissionCardiovascular', 'passthrough',  []),\n",
    "    ('_admissionRespiratory',    'passthrough',  []),\n",
    "    ('_admissionNeurologic',     'passthrough',  []),\n",
    "  \n",
    "    # CarePlanProvider\n",
    "    ('Categoria_Hospital',          'passthrough', []), \n",
    "    ('Categoria_Cardiology',        'passthrough', []),\n",
    "    ('Categoria_Internal_Medicine', 'passthrough', []),\n",
    "    ('Intervencion_I',              'passthrough', []),\n",
    "    ('Intervencion_II',             'passthrough', []),\n",
    "    ('Intervencion_III',            'passthrough', []), \n",
    "    ('Intervencion_IV',             'passthrough', []),\n",
    "    \n",
    "    # CarePlanGeneral\n",
    "    ('Categoria_Ventilacion',              'passthrough', []),\n",
    "    ('Categoria_DVT_Prophylaxis',          'passthrough', []),\n",
    "    ('Categoria_Airway',                   'passthrough', []),\n",
    "    ('Categoria_Care_Limitation',          'passthrough', []),\n",
    "    ('Categoria_Stress_Ulcer_Prophylaxis', 'passthrough', []),\n",
    "    \n",
    "    # CarePlanInfectiousDisease\n",
    "    ('infectdiseasesite',      OneHotEncoder(categories=[categs['infectdiseasesite']]), []),\n",
    "    ('infectdiseaseassessment', OneHotEncoder(categories=[categs['infectdiseaseassessment']]), []),\n",
    "    \n",
    "    ('last_aperiodic_off',       'passthrough', []),\n",
    "    ('last_aperiodic_systolic',  'passthrough', []),\n",
    "    ('last_aperiodic_diastolic', 'passthrough', []),\n",
    "    ('last_aperiodic_mean',      'passthrough', []),\n",
    "    \n",
    "    # vitalPeriodic\n",
    "    ('VP_temp',        'passthrough', []),\n",
    "    ('VP_sao2',        'passthrough', []),\n",
    "    ('VP_respiration', 'passthrough', []),\n",
    "    ('VP_cvp',         'passthrough', []),\n",
    "    ('VP_heartrate',   'passthrough', []),\n",
    "\n",
    "    # Medication\n",
    "    ('last_order_offset', 'passthrough', []),\n",
    "    \n",
    "    # Allergy\n",
    "    # TODO\n",
    "    ('nDrugsAllergic',    'drop', []),#OneHotEncoder(categories=[categs['nDrugsAllergic']]), []),\n",
    "    ('nNondrugsAllergic', 'drop', []),#OneHotEncoder(categories=[categs['nNondrugsAllergic']]), []),\n",
    "    ('totalAllergic',     'drop', []),#OneHotEncoder(categories=[categs['totalAllergic']]), []),\n",
    "    \n",
    "    # TODO\n",
    "    ('asdofibasodfboasdfdsf',  'passthrough', []),\n",
    "    \n",
    "]\n",
    "\n",
    "# Numerar columnas para el tranformador\n",
    "for i in range(len(transformers)):\n",
    "    transformers[i][2].append(i)\n",
    "\n",
    "# Transformar la matriz\n",
    "X_T_train = ColumnTransformer(transformers=transformers).fit_transform(X_train)\n",
    "X_T_test  = ColumnTransformer(transformers=transformers).fit_transform(X_test)\n",
    "\n",
    "# Mostrar el cambio en columnas\n",
    "print('Train', X.shape, '->', X_T_train.shape)\n",
    "print('Test ', X_test.shape, ' ->', X_T_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1df4b7-8b4f-4314-8da5-b3fa9c31b62a",
   "metadata": {},
   "source": [
    "<center><h2><b>Busqueda de Hiperparametros</b></h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eee9c46-79c8-49c9-8391-29311f61e5d3",
   "metadata": {
    "tags": []
   },
   "source": [
    "Hemos usado Nevergrad para buscar los hiperpar√°metros de RandomForest y XBoost mediante un algor√≠tmo gen√©tico, ya que son una cantidad importante de hiperpar√°metros.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f505075b-f1c9-4775-8468-f720c6b76ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N√∫mero aleatorio para la b√∫squeda de hiperpar√°metros\n",
    "rand_n = random.randint(0, 100000000000)\n",
    "\n",
    "# Funcion auxiliar para devolver un conjunto de scores y su media y desviaci√≥n estandar\n",
    "def cv_avg_std(reg, X, y, scoring):\n",
    "    maes = cross_val_score(reg, X, y, cv=5, scoring=scoring)\n",
    "    avg = stat.mean(maes)\n",
    "    std_dev = stat.variance(maes)**(1/2)\n",
    "    \n",
    "    return maes, avg, std_dev\n",
    "\n",
    "# Funci√≥n de optimizaci√≥n para nevergrad\n",
    "def optimize_spectral(n_estimators, min_samples_leaf, min_samples_split, max_depth, max_features, warm_start, bootstrap):\n",
    "    reg = RandomForestRegressor(n_estimators=n_estimators, min_samples_leaf=min_samples_leaf,\n",
    "                              min_samples_split=min_samples_split, max_depth=max_depth, random_state=rand_n,\n",
    "                              max_features=max_features, bootstrap=bootstrap)\n",
    "\n",
    "    maes, avg, std_dev = cv_avg_std(reg, X_T_train, y_train, 'neg_mean_absolute_error')\n",
    "    _, r2, __ = cv_avg_std(reg, X_T, y, 'r2')\n",
    "  \n",
    "    print('AVG_MAE', avg, 'R2', r2, 'std_dev', std_dev, \"[\", n_estimators, \",\", min_samples_leaf, \",\", min_samples_split, \",\", max_depth, ',', max_features, ',', warm_start, ',', bootstrap, '] - ', rand_n)\n",
    "\n",
    "    return float('inf') if std_dev > 75 else -r2\n",
    "\n",
    "instru = ng.p.Instrumentation(\n",
    "    ng.p.Choice([x for x in range(40, 200)]), # n_estimators\n",
    "    ng.p.Choice([x for x in range(1, 4)]), # min_samples_leaf\n",
    "    ng.p.Choice([x for x in range(2, 5)]), # min_samples_split\n",
    "    ng.p.Choice([x for x in range(30, 200)]), # max_depth\n",
    "    ng.p.Choice([x for x in range(10, 200)]), # max_features\n",
    "    ng.p.Choice([True, False]), # warm_start\n",
    "    ng.p.Choice([True, False]) # bootstrap\n",
    ")\n",
    "\n",
    "# Descomentar para ejecutar\n",
    "\n",
    "#optimizer = ng.optimizers.CM(parametrization=instru, budget=1000)\n",
    "#recommendation = optimizer.minimize(optimize_spectral)\n",
    "#print(recommendation.value)  # recommended value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e91faa-9481-49ca-a7ba-e48e37cf9def",
   "metadata": {},
   "source": [
    "<center><h2><b>Reducci√≥n dimensional</b></h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2524d57e-845e-4fd0-93a0-e6bf4cd132df",
   "metadata": {
    "tags": []
   },
   "source": [
    "Hemos probado SVD para reducir el n√∫mero de features, acelerar el tiempo de entreno y reducir el ruido que se haya podido originar<br>\n",
    "por parte de features que no sean demasiados √∫tiles, ya que nuestro conjunto de training es una matriz esparsa y no se puede usar PCA,<br>\n",
    "sin embargo, no ha ayudado a mejorar el modelo, incrementando en su vez todas las medidas de error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7760a0-827c-4de4-95b0-d400492d2a03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def feature_n_for_explained_variance_ratio(svd, ratio):\n",
    "    total = 0\n",
    "    \n",
    "    for i, n in enumerate(svd.explained_variance_ratio_):\n",
    "        total += n\n",
    "        \n",
    "        if total >= ratio:\n",
    "            return i+1\n",
    "    \n",
    "    return -1\n",
    "\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=22)\n",
    "_X_T_train = svd.fit_transform(X_T_train)\n",
    "\n",
    "for n in (0.5, 0.75, 0.9, 0.99, 0.999, 0.9999):\n",
    "    print(\"Numero de features para explicar un \" + str(n*100) + \"% de la varianza:\",\n",
    "          feature_n_for_explained_variance_ratio(svd, n))\n",
    "\n",
    "#X_T = svd.transform(X_T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9209c1e-dcc1-48f3-a9c2-ee1fda740e38",
   "metadata": {
    "tags": []
   },
   "source": [
    "<center><h2><b>Entrenamiento y calcular Error</b></h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96666a24-5ac2-4dbf-a7ca-68a31b809077",
   "metadata": {},
   "source": [
    "Para el c√°lculo de error hemos usado tres medidas. Por una parte tenemos el MAE para calcular la distancia media a la variable objetivo que es el<br>\n",
    "n√∫mero de minutos que tenemos que predecir que un paciente va a permanecer en la UCI. Aparte del MAE tambi√©n hemos considerado importante la desviaci√≥n<br>\n",
    "estandar del mismo ya que se trata de un sistema donde se pueden perder vidas si se hacen predicciones poco precisas. Por √∫ltimo hemos usado R2 para<br>\n",
    "calcular la cantidad de varianza que explica el modelo y evitar overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbff6c5b-7009-4365-8195-93d253660201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg df: 3484.0752 = 2.420139 dias.\n",
    "def cv_avg_std(reg, X, y, scoring):\n",
    "    maes = cross_val_score(reg, X, y, cv=2, scoring=scoring)\n",
    "    avg = stat.mean(maes)\n",
    "    std_dev = stat.variance(maes)**(1/2)\n",
    "    \n",
    "    return maes, avg, std_dev\n",
    "\n",
    "def make_df(datos_reg):\n",
    "    error_df = pd.DataFrame()\n",
    "\n",
    "    error_df['Regresor']               = datos_reg.keys()\n",
    "    error_df['Average MAE']            = [ abs(dato['avg']) for dato in datos_reg.values() ]\n",
    "    error_df['Standard Deviation MAE'] = [ dato['std_dev'] for dato in datos_reg.values() ]\n",
    "    error_df['Average R2']             = [ dato['avg_r2'] for dato in datos_reg.values() ]\n",
    "    error_df['time']                   = [ dato['time'] for dato in datos_reg.values() ]\n",
    "    \n",
    "    return error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3851ab3e-016b-4678-816d-221e81deea67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Linear regression \n",
    "lreg = LinearRegression()\n",
    "\n",
    "rfreg = RandomForestRegressor(\n",
    "        n_estimators = 121,\n",
    "        min_samples_leaf = 2,\n",
    "        min_samples_split = 3,\n",
    "        max_depth = 91,\n",
    "        max_features = 189,\n",
    "        warm_start = False,\n",
    "        bootstrap = False,\n",
    "        random_state = 14684358)\n",
    "\n",
    "xboostreg = XGBRegressor()\n",
    "\n",
    "votingreg = VotingRegressor([('xboost', lreg), ('rfreg', rfreg)], weights=[1, 3])\n",
    "\n",
    "datos_reg = {}\n",
    "regressors = [\n",
    "    ('Lineal', lreg),\n",
    "    ('Random Forest', rfreg),\n",
    "    ('XGBRegressor', xboostreg),\n",
    "    ('VotingRegressor', votingreg),\n",
    "]\n",
    "\n",
    "# Medir tiempo y hacer predicciones para cada regresor\n",
    "for reg_name, reg in regressors:\n",
    "    start_time = time.time()\n",
    "\n",
    "    maes, avg, std_dev = cv_avg_std(reg, X_T_train, y_train, 'neg_mean_absolute_error')\n",
    "    maes, r2, _ = cv_avg_std(reg, X_T_train, y_train, 'r2')\n",
    "    \n",
    "    datos_reg[reg_name] = { 'avg': avg, 'std_dev': std_dev, 'time': time.time() - start_time }\n",
    "    datos_reg[reg_name]['avg_r2'] = r2\n",
    "\n",
    "df_reg = make_df(datos_reg)\n",
    "df_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1963dca0-6d5d-4399-96a5-b0a7a22ba6b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.set_palette(\"Paired\")\n",
    "fig, axs = plt.subplots(ncols=3, nrows=1,figsize=(20, 5))\n",
    "\n",
    "sns.barplot(data=df_reg, x='Regresor', y='Average MAE', ax=axs[0])\n",
    "sns.barplot(data=df_reg, x='Regresor', y='Standard Deviation MAE', ax=axs[1])\n",
    "sns.barplot(data=df_reg, x='Regresor', y='Average R2', ax=axs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e807b882-d8c7-4340-b163-9a3afb75fac3",
   "metadata": {},
   "source": [
    "Por √∫ltimo hacemos un plot con el error de cada modelo para comparar cuanto se distancian de un resultado ideal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9797b3b-863b-408e-acf6-5396d6396a3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rfreg.fit(X_T_train, y_train)\n",
    "pred = rfreg.predict(X_T_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad9de5d-730f-4b99-a3fa-a41e44b9a35b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# avg df: 3484.0752 = 2.420139 dias.\n",
    "mae = mean_absolute_error(y_test, pred)\n",
    "r2  = r2_score(y_test, pred)\n",
    "\n",
    "print('MAE test:', mae)\n",
    "print('R2:', r2, '\\n')\n",
    "\n",
    "err = prediction_error(votingreg,  X_T_train, y_train, X_T_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1827f23f-1382-45ea-b046-33ae71627c63",
   "metadata": {},
   "source": [
    "Basandonos en estos plots podemos ver que todos se acercan bastante a la pendiente de 45¬∫. LinearRegressor y GBRegressor se<br>\n",
    "alejan un poco m√°s, pero es natural ya que no hemos tenido tiempo de buscar unos hiperpar√°metros razonables o hacer eliminaci√≥n<br>\n",
    "de outliers. En el caso de RandomForest no le afectan tanto estos outliers y se hace posible una explicaci√≥n m√°s amplia de la varianza."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe20616-c37b-43a9-bc7c-d0766612916e",
   "metadata": {},
   "source": [
    "---\n",
    "<center><h2><b>Conclusi√≥n</b></h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a865ba4d-3e70-4bf0-890b-5ecda6a6d9bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "Tras evaluar todos estos modelos podemos observar que mientras que LinearRegressor tiene un ligeramente mayor R2 y un tiempo de ejecuci√≥n<br>\n",
    "envidiable, la diferencia en MAE no compensa usarlo. RandomForestRegressor se muestra como un contendiente fuerte, pero hemos podido mejorarlo<br>\n",
    "haciendo un ensemble de Voting con el anterior. XBoost probablemente mejorar√≠a dado el tiempo suficiente para hacer una b√∫squeda de hiperpar√°metros<br>\n",
    "exhaustiva, pero no hemos tenido tiempo. A todo esto pensamos que el VotingRegressor muestra unos resultados aceptables.<br>\n",
    "\n",
    "En un nivel m√°s abstracto podemos ver que tiene una media de error de ~262 minutos ~ 4 horas y una desviaci√≥n estandar de alrededor de media hora. Creemos<br>\n",
    "que es importante contar con una desviaci√≥n estandar peque√±a para poder predecir la estancia de los pacientes con cierta fiabilidad y teniendo en cuenta<br>\n",
    "que el tiempo medio que un paciente est√° en la UCI en E.E.U.U es de alrededor de 10 d√≠as y que la media del dataset es de 2.4 d√≠as, creemos que es un<br>\n",
    "resultado interesante con el cual se podr√≠an hacer, hipot√©ticamente, predicciones √∫tiles.<br>\n",
    "\n",
    "Tambi√©n es merecedor de menci√≥n que este resultado no supera a APACHE. Un modelo √∫nicamente entrenado con la variable unbridgedunitlos que tiene un √≠ndice<br>\n",
    "de correlaci√≥n de m√°s del 98% con la variable que estamos buscando tiene un R2 de un 98.5% y un MAE y desviaci√≥n estandar menores. No hemos querido mejorar<br>\n",
    "el modelo hasta esos niveles tanto por falta de tiempo como por miedo de overfitting, aunque Random Forest es notorio por su tendencia a evitar overfitting.<br>\n",
    "\n",
    "En general y para terminar podemos decir que ha sido un reto interesante donde nuestros principales problemas han sido la interpretaci√≥n de 30 tablas de datos<br>\n",
    "(muchas para la experiencia que hab√≠amos tenido hasta ahora) y la interpretaci√≥n de dichos datos, sobretodo los datos vac√≠os (donde algunos han acabado teniendo<br>\n",
    "sentido) como el significado de dichos datos en un contexto m√©dico."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
